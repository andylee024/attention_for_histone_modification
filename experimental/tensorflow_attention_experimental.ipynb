{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow Attention Experimental\n",
    "This notebook provides a rough implementation of the attention model for histone modification prediction in tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "This section lists out the important hyperparameters used in our prediction task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N = 100  # batch size\n",
    "L = 196  # number of annotation vectors per training example\n",
    "D = 500  # dimension of each annotation vector\n",
    "H = 100  # number of hidden units\n",
    "T = 400  # length of sequence\n",
    "V = 4    # vocabulary size ('a', 'c', 'g', 't')\n",
    "C = 3    # number of prediction classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities\n",
    "This section provides utilities for converting between tensor objects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_label_to_one_hot(label, number_of_classes):\n",
    "    \"\"\"Converts a discrete label to one hot encoding.\n",
    "    \n",
    "    @param labels: numpy array of discrete labels \n",
    "    @param number_of_classes: number of label classes\n",
    "    @return: 1xC one-hot encoding, where C is number of classes\n",
    "    \"\"\"\n",
    "    one_hot_encoding = np.zeros(number_of_classes)\n",
    "    one_hot_encoding[label] = 1\n",
    "    return np.reshape(one_hot_encoding, newshape=(1, number_of_classes))\n",
    "\n",
    "def convert_to_one_hot(labels, number_of_classes):\n",
    "    \"\"\"Convert a list of labels to one hot encoding.\n",
    "    \n",
    "    @param labels: numpy array of discrete labels \n",
    "    @param number_of_classes: number of label classes\n",
    "    @return: one-hot encoding of lables (N x C), where N is batch size, C is number of classes\n",
    "    \"\"\"\n",
    "    one_hot_labels = [convert_label_to_one_hot(l, number_of_classes=number_of_classes) for l in labels]\n",
    "    return np.concatenate(one_hot_labels, axis=0)\n",
    "\n",
    "def convert_training_examples_to_tensor(training_examples):\n",
    "    \"\"\"Convert batch data to tensor representation.\n",
    "    \n",
    "    @param training_examples:\n",
    "        List of training examples.\n",
    "    @return: \n",
    "        Numpy tensors of dimension (N x (A1 x A2)), where N is batch dimension and (A1 x A2) is dimension of \n",
    "        matrix corresponding to training example\n",
    "    \"\"\"\n",
    "    # Add batch dimension to tensors and concatenate matrices across batch dimension\n",
    "    # np.expand_dims - adds batch dimension\n",
    "    # np.concatenate - creates batch tensor by stacking on batch dimension\n",
    "    sequence_tensor = np.concatenate([np.expand_dims(te.sequence, axis=0) for te in training_examples], axis=0)\n",
    "    annotation_tensor = np.concatenate([np.expand_dims(te.annotation_vectors, axis=0) for te in training_examples], axis=0)\n",
    "    label_tensor = np.concatenate([np.expand_dims(te.label, axis=0) for te in training_examples], axis=0)\n",
    "    label_tensor = np.ravel(label_tensor)\n",
    "    \n",
    "    return TrainingTensor(sequence_tensor=sequence_tensor, \n",
    "                          annotation_tensor=annotation_tensor, \n",
    "                          label_tensor=label_tensor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mock Data\n",
    "This section provides utilities for mocking the data that will be used in our prediction task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "TrainingExample = collections.namedtuple(\n",
    "    typename='TrainingExample', field_names=['sequence', 'annotation_vectors', 'label'])\n",
    "\n",
    "TrainingTensor = collections.namedtuple(\n",
    "    typename=\"TrainingTensor\", field_names=['sequence_tensor', 'annotation_tensor', 'label_tensor'])\n",
    "\n",
    "def create_dummy_training_example():\n",
    "    \"\"\"Create a single training example with dummy data.\"\"\"\n",
    "    dummy_sequence = convert_to_one_hot(labels=np.random.randint(low=0, high=V, size=T), number_of_classes=V)\n",
    "    dummy_label = np.random.randint(low=0, high=3, size=1)\n",
    "    dummy_annotation_vectors = np.random.normal(loc=0.0, scale=1.0, size=(L, D))\n",
    "    \n",
    "    return TrainingExample(sequence=dummy_sequence,\n",
    "                           annotation_vectors=dummy_annotation_vectors,\n",
    "                           label=dummy_label)    \n",
    "\n",
    "def create_dummy_batch_data():\n",
    "    \"\"\"Create training examples for batch.\"\"\"\n",
    "    training_examples = [create_dummy_training_example() for _ in xrange(N)]\n",
    "    return convert_training_examples_to_tensor(training_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 400, 4)\n",
      "(100, 196, 500)\n",
      "(100,)\n"
     ]
    }
   ],
   "source": [
    "batch_data = create_dummy_batch_data()\n",
    "\n",
    "# print tensor dimensions for reference\n",
    "print batch_data.sequence_tensor.shape\n",
    "print batch_data.annotation_tensor.shape\n",
    "print batch_data.label_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Attention Model\n",
    "This section builds the computational graph (i.e. model) for the attention model. \n",
    "We will need to implement the following layers and operations.\n",
    "\n",
    "1. Get initial LSTM state (compute initial LSTM state given features)\n",
    "2. Process sequential input to get hidden state.\n",
    "3. Attention layer (compute attention probabilities given state and features)\n",
    "3. Get context vector\n",
    "4. Perform prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weight_initializer = tf.contrib.layers.xavier_initializer()\n",
    "constant_initializer = tf.constant_initializer(0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial LSTM state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_initial_lstm(features, reuse):\n",
    "    \"\"\"Returns initial state of LSTM by initializing with CNN features.\n",
    "    \n",
    "    Input: features (N x L x D)\n",
    "    Output: hidden_state (N x H), memory_state (N x H)\n",
    "    \n",
    "    Note that we want to separately initialize the hidden state for each sequence\n",
    "    because we assume that the sequences are independent. We do \n",
    "    not want the state information from a previous sequence to leak into the current sequence.\n",
    "    \n",
    "    :param features:\n",
    "        Features extracted from CNN of dimension (L x D).\n",
    "    :return: \n",
    "        initial hidden and memory state.\n",
    "    \"\"\"\n",
    "    features_mean = tf.reduce_mean(features, axis=1) # (N x D)\n",
    "    \n",
    "    with tf.variable_scope('initial_lstm', reuse=reuse):\n",
    "        \n",
    "        # get initial hidden state\n",
    "        w_h = tf.get_variable('w_h', shape=(D, H), initializer=weight_initializer)\n",
    "        b_h = tf.get_variable('b_h', shape=(H), initializer=constant_initializer)\n",
    "        h_init_logits = tf.matmul(features_mean, w_h) + b_h\n",
    "        h_init = tf.nn.tanh(h_init_logits)\n",
    "        \n",
    "        # get initial memory state\n",
    "        w_c = tf.get_variable('w_c', shape=(D, H), initializer=weight_initializer)\n",
    "        b_c = tf.get_variable('b_c', shape=(H), initializer=constant_initializer)\n",
    "        c_init_logits = tf.matmul(features_mean, w_c) + b_c\n",
    "        c_init = tf.nn.tanh(c_init_logits)\n",
    "        \n",
    "        return h_init, c_init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def attention_project_features(features, reuse=False):\n",
    "    \"\"\"Apply weighted transformation to features.\n",
    "    \n",
    "    Input: (N x L x D) - all annotation vectors for each batch entry\n",
    "    Output: (N x L x D) - projected annotation vectors for each batch entry\n",
    "    \"\"\"\n",
    "    with tf.variable_scope('attention_project_features', reuse=reuse):\n",
    "        features_flat = tf.reshape(features, [-1, D]) # (NL x D)\n",
    "        w_features = tf.get_variable('w_features', [D, D], initializer=weight_initializer)\n",
    "        projected_features = tf.matmul(features_flat, w_features) # (NL x D)\n",
    "        projected_features = tf.reshape(projected_features, [N, L, D]) # (N x L x D)\n",
    "        return projected_features\n",
    "    \n",
    "def attention_project_hidden_state(h, reuse=False):\n",
    "    \"\"\"Apply weighted transformation to hidden state.\n",
    "     \n",
    "    Input: (N x H) - hidden state for each batch entry\n",
    "    Output: (N x D) - projected hidden state for each batch entry\n",
    "    \"\"\"\n",
    "    with tf.variable_scope('attention_project_hidden_state', reuse=reuse):\n",
    "        w_hidden = tf.get_variable('w_hidden', [H, D], initializer=weight_initializer)\n",
    "        projected_h = tf.matmul(h, w_hidden) \n",
    "        return projected_h\n",
    "\n",
    "def attention_bias(reuse=False):\n",
    "    \"\"\"Get attention bias.\n",
    "    \n",
    "    Output: (H x 1)\n",
    "    \"\"\"\n",
    "    with tf.variable_scope('attention_bias', reuse=False):\n",
    "        b = tf.get_variable('b', [D], initializer=constant_initializer)\n",
    "        return b\n",
    "    \n",
    "def get_attention_inputs(features, h):\n",
    "    # transform features (N x L x D)\n",
    "    projected_features = attention_project_features(features)\n",
    "        \n",
    "    # transform hidden state (N x 1 x D)\n",
    "    projected_h = tf.transpose(attention_project_hidden_state(h))\n",
    "    projected_h = tf.reshape(projected_h, shape=[N, 1, D])\n",
    "        \n",
    "    # get bias\n",
    "    bias = attention_bias()\n",
    "    \n",
    "    return projected_features, projected_h, bias\n",
    "\n",
    "def attention_layer(features, h, reuse=False):\n",
    "    \"\"\"Returns attention probabilities. \n",
    "    \n",
    "    Input: \n",
    "        1. (N x L x D) - features for each batch entry\n",
    "        2. (N x H) - hidden state for each batch entrying respective sequences.\n",
    "    \n",
    "    Output: \n",
    "        1. (N x L) matrix of probabilities for each annotation vector in each batch entry\n",
    "    \"\"\"\n",
    "    with tf.variable_scope('attention_layer', reuse=reuse):\n",
    "        projected_features, projected_h, bias = get_attention_inputs(features, h)\n",
    "        \n",
    "        # create attention input\n",
    "        # note that +bias is a broadcasted operation\n",
    "        attention_input = projected_features + projected_h #+ bias # (N x L x D)\n",
    "        attention_input = tf.reshape(attention_input, shape=[-1, D]) # (NL x D)\n",
    "        \n",
    "        # apply attention mechanism\n",
    "        w_attention = tf.get_variable('w_attention', shape=[D, 1], initializer=weight_initializer)\n",
    "        attention_logits = tf.matmul(attention_input, w_attention) # (NL x 1)\n",
    "        attention_logits = tf.reshape(attention_logits, shape=(N, L)) # (N x L)\n",
    "        \n",
    "        # compute attention probabilties\n",
    "        attention_probabilities = tf.nn.softmax(attention_logits) # (N x L)\n",
    "        return attention_probabilities "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def convert_to_gather_indices(selected_indices):\n",
    "    \"\"\"Convert selected context indices to tensor to be used for gather_nd.\"\"\"\n",
    "    indices = tf.reshape(np.arange(N), shape=(N,1))\n",
    "    selected_context_indices = tf.reshape(selected_indices, shape=(N,1))\n",
    "    return tf.concat((indices, selected_context_indices), axis=1)\n",
    "\n",
    "def select_context(features, attention_probabilities):\n",
    "    \"\"\"Select context vector from attention probabilities\n",
    "    \n",
    "    :param features:\n",
    "        (N x L x D) tensor, where N is batch size, L is number of attention\n",
    "        vectors and D is dimension of attention vector. \n",
    "    :param attention_probabilities:\n",
    "        (N x L) tensor of probabilities.\n",
    "    :return:\n",
    "        (N x D), where each row represents a context vector for ith example.\n",
    "    \"\"\"\n",
    "    selected_context_indices = tf.argmax(attention_probabilities, axis=1)\n",
    "    gather_indices = convert_to_gather_indices(selected_context_indices)\n",
    "    return tf.gather_nd(params=features, indices=gather_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decode_lstm(hidden_state, context, reuse=False):\n",
    "    \"\"\"Predict on hidden state and context.\"\"\"\n",
    "    with tf.variable_scope('decode_lstm', reuse=reuse):\n",
    "        w_hidden = tf.get_variable('w_hidden', [H, C], initializer=weight_initializer)\n",
    "        w_context = tf.get_variable('w_context', [D, C], initializer=weight_initializer)\n",
    "        b_out = tf.get_variable('b_out', [C], initializer=constant_initializer)\n",
    "        \n",
    "        hidden_contribution = tf.matmul(hidden_state, w_hidden)\n",
    "        context_contribution = tf.matmul(context, w_context)\n",
    "        logits =  hidden_contribution + context_contribution + b_out\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Model (Computational Graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model():\n",
    "    loss = 0.0\n",
    "    \n",
    "    # inputs\n",
    "    features = tf.placeholder(tf.float32, (None, L, D))\n",
    "    sequences = tf.placeholder(tf.float32, (None, T, V))\n",
    "    labels = tf.placeholder(tf.int32, (None))\n",
    "    \n",
    "    inputs = {'features': features, \n",
    "              'sequences': sequences,\n",
    "              'labels': labels}\n",
    "    \n",
    "    # initialization\n",
    "    c, h = get_initial_lstm(features, reuse=False)\n",
    "    loss = 0.0\n",
    "    \n",
    "    lstm_cell = tf.contrib.rnn.BasicLSTMCell(num_units=H)\n",
    "    \n",
    "    # process sequence to get updated hidden unit\n",
    "    for t in range(T):\n",
    "        with tf.variable_scope('update_lstm', reuse=(t!=0)):\n",
    "            _, (c,h) = lstm_cell(inputs=sequences[:, t, :], state=[c, h])\n",
    "    \n",
    "    # get context\n",
    "    attention_probabilities = attention_layer(features, h, reuse=None) # (N x L)\n",
    "    context = select_context(features, attention_probabilities) # (N x D)\n",
    "    \n",
    "    # get logits\n",
    "    logits = decode_lstm(h, context, reuse=None)\n",
    "    \n",
    "    # get loss\n",
    "    loss += tf.reduce_sum(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=logits))\n",
    "\n",
    "    # return loss\n",
    "    return (loss / tf.to_float(N)), inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the network\n",
    "This section puts together the function needed to train the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the loss for iteration 0 = 1.5430419445\n",
      "the loss for iteration 1 = 1.50749766827\n",
      "the loss for iteration 2 = 1.5101596117\n",
      "the loss for iteration 3 = 1.66599965096\n",
      "the loss for iteration 4 = 1.75353240967\n",
      "the loss for iteration 5 = 1.69154560566\n",
      "the loss for iteration 6 = 1.72667145729\n",
      "the loss for iteration 7 = 1.68650984764\n",
      "the loss for iteration 8 = 1.4467086792\n",
      "the loss for iteration 9 = 1.37894380093\n"
     ]
    }
   ],
   "source": [
    "# training hyperparameters\n",
    "learning_rate = 0.01\n",
    "number_epochs = 1\n",
    "number_iterations = 10\n",
    "\n",
    "# build model\n",
    "loss, model_inputs = model()\n",
    "\n",
    "# train op\n",
    "with tf.name_scope('optimizer'):\n",
    "    #optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=1e-4)\n",
    "    train_op = optimizer.minimize(loss)\n",
    "\n",
    "# add initialization operations \n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "# reuse variables\n",
    "tf.get_variable_scope().reuse_variables()\n",
    "\n",
    "with tf.Session() as sess:    \n",
    "    sess.run(init_op)\n",
    "    for e in range(number_epochs):\n",
    "        for i in range(number_iterations):\n",
    "            batch_data = create_dummy_batch_data()\n",
    "            \n",
    "            feed_dict = {model_inputs['sequences']: batch_data.sequence_tensor,\n",
    "                         model_inputs['features']: batch_data.annotation_tensor,\n",
    "                         model_inputs['labels']: batch_data.label_tensor}\n",
    "            \n",
    "            _, loss_value = sess.run([train_op, loss], feed_dict)\n",
    "            print \"the loss for iteration {} = {}\".format(i, loss_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
