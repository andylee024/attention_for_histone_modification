import abc
import tensorflow as tf

from komorebi.libs.trainer.trainer_config import TrainerConfiguration
from komorebi.libs.trainer.trainer_utils import batch_data

class AbstractTrainer(object):
    """Abstract base class for model trainer."""
    __metaclass__ = abc.ABCMeta
   
    @abc.abstractmethod
    def train_model(model, dataset, *args, **kwargs):
        """Train a model on a dataset.

        :param model: model object satisfying abstract model interface
        :param dataset: dataset object satisfying abstract dataset interface
        :return: trained model 
        """
        pass


class AbstractTensorflowTrainer(AbstractTrainer):
    """Abstract base class for model trainer specific to tensorflow."""
    __metaclass__ = abc.ABCMeta

    def __init__(self, config):
        """Initialize trainer.
        
        :param config: trainer_config object.
        """
        assert isinstance(config, TrainerConfiguration)
        self.epochs = config.epochs
        self.batch_size = config.batch_size

    def train_model(self, model, dataset, optimizer):
        """Train a model on a dataset.

        The default behavior is to call the train and loss op, but this method can be 
        overridden for derived classes.
        
        :param model: model object satisfying abstract model interface
        :param dataset: dataset object satisfying abstract dataset interface
        :return: trained model 
        """
        graph_inputs, ops = self._build_computational_graph(model, optimizer)

        ## initialize variables
        init_op = tf.global_variables_initializer()
        tf.get_variable_scope().reuse_variables()

        ## initialize session and start training
        with tf.Session() as sess:
            sess.run(init_op)

            # debug code
            from tensorflow.python import debug as tf_debug
            sess = tf_debug.LocalCLIDebugWrapperSession(sess)
            
            for _ in xrange(self.epochs):
                training_batches = batch_data(dataset, batch_size=self.batch_size)
                training_batch = training_batches.next()
                _, loss = sess.run(
                        fetches=[ops['train_op'], ops['loss_op']], 
                        feed_dict=self._convert_training_examples_to_feed_dict(graph_inputs, training_batch))
                


            #for _ in xrange(self.epochs):
            #    training_batches = batch_data(dataset, batch_size=self.batch_size)
                #for idx, training_batch in enumerate(training_batches):
                #    _, loss = sess.run(
                #            fetches=[ops['train_op'], ops['loss_op']], 
                #            feed_dict=self._convert_training_examples_to_feed_dict(graph_inputs, training_batch))
                #    print "the loss for iteration {} = {}".format(idx, loss)

                


    @abc.abstractmethod
    def _build_computational_graph(self, model, optimizer):
        """Construct a computational graph for training a model.

        :param model: tensorflow model to be trained
        :param optimizer: optimizer for gradient backpropogation
        :return: 
            2-tuple consisting the two dictionaries. The first dictionary contains tf.placeholders
            representing inputs to the graph. The second dictionary contains ops generated by the graph.
        """
        pass


    @abc.abstractmethod
    def _convert_training_examples_to_feed_dict(self, graph_inputs, training_examples):
        """Convert training inputs to graph inputs.

        Tensorflow models rely on passing a feed_dict into the computational graph.
        This function is responsible for translating the training examples into graph inputs.

        :param graph_inputs: dictionary mapping from string key to tf.placeholders
        :param training_examples: training example types specific to dataset.
        """
        pass


        

