import collections
import tensorflow as tf

from komorebi.libs.trainer.abstract_tensorflow_trainer import AbstractTensorflowTrainer
from komorebi.libs.trainer.trainer_utils import infer_positive_upweight_parameter
from komorebi.libs.utilities.constants import SINGLE_PREDICTION

# Struct for holding single task data
single_task_data_t = collections.namedtuple(typename="single_task_data_t", field_names=["labels", "logits"])

# Struct for storing debug ops that are helpful for examining model training
debug_data_t = collections.namedtuple(typename="debug_data_t", 
        field_names=['training_loss', 'single_task_labels', 'single_task_logits', 'multitask_labels'])

class AttentionTrainer(AbstractTensorflowTrainer):
    """Trainer implementation for training attention models."""

    def __init__(self, task_index, trainer_config, experiment_directory, checkpoint_directory, summary_directory, logger):
        """Initialize trainer.

        :param task_index: the task index on which to train model
        :param config: trainer_config object.
        :param experiment_directory: directory corresponding to experiment
        :param checkpont_directory: directory for storing model checkpoints
        :param summary_directory: directory for storing tf summaries
        :param logger: logger object
        """
        super(AttentionTrainer, self).__init__(config=trainer_config,
                                               experiment_directory=experiment_directory,
                                               checkpoint_directory=checkpoint_directory,
                                               summary_directory=summary_directory, 
                                               logger=logger)
        self.task_index = task_index
        self.logger.info("Training on task index : {}".format(self.task_index))

    def _build_computational_graph(self, dataset, model, optimizer):
        """Construct a computational graph for training a model.
    
        :param model: tensorflow model to be trained
        :param optimizer: optimizer for gradient backpropogation
        :return: 
            2-tuple consisting the two dictionaries. The first dictionary contains tf.placeholders
            representing inputs to the graph. The second dictionary contains ops generated by the graph.
        """
        positive_upweight = _infer_training_parameters(dataset=dataset, task_index=self.task_index)
        self.logger.info("Task[{}] positive upweight parameter: {}".format(self.task_index, positive_upweight))

        graph_inputs = {'features'  : model.inputs['features'],
                        'sequence' : model.inputs['sequence'],
                        'labels'    : model.outputs['labels']}

        single_task_data = _prepare_single_task_data(
                task_index=self.task_index, logits=model.inference['logit'], labels=graph_inputs['labels'])

        loss_op = _get_loss_op(predictions=single_task_data.logits, 
                               labels=single_task_data.labels, 
                               positive_upweight=positive_upweight)

        train_op = _get_train_op(loss_op=loss_op, optimizer=optimizer)
        summary_op = _get_summary_op(loss_op)

        debug_op = debug_data_t(single_task_labels=single_task_data.labels, 
                                single_task_logits=single_task_data.logits,
                                multitask_labels=model.outputs['labels'],
                                training_loss=loss_op)

        ops = {'loss_op' : loss_op, 'train_op': train_op, 'summary_op': summary_op, 'debug_op': debug_op}
        return graph_inputs, ops


    def _convert_training_examples(self, data, graph_inputs):
        """Convert training examples to graph inputs.
        
        :param data: tf examples parsed from dataset
        :param graph_inputs: dictionary mapping from string key to tf.placeholders
        :return: 
            feed_dict dictionary where keys are tf.placeholders and values are tensors.
            This dictionary is passed to computational graph.
        """
        return {graph_inputs['sequence']: data['sequence'],
                graph_inputs['features']: data['annotation'],
                graph_inputs['labels']: data['label']}


def _prepare_single_task_data(task_index, logits, labels):
    """Extract relevant single task labels from the original dataset labels and make data shapes consistent.

    :param task_index: the task index corresponding to single task prediction
    :param logits: tensor ouput from model.
    :param labels: groundtruth labels from original dataset
    :return: single_task_data_t object
    """
    single_task_labels = tf.reshape(labels[:, task_index], [-1, SINGLE_PREDICTION])
    logits = tf.reshape(logits, [-1, SINGLE_PREDICTION])
    return single_task_data_t(labels=single_task_labels, logits=logits)


def _get_loss_op(predictions, labels, positive_upweight):
    """Return loss for model.

    :param predictions: tensor ouput from model (currently logits)
    :param labels: groundtruth labels for single task prediction
    :param positive_upweight: upweight parameter for positive samples
    :return: loss
    """
    with tf.name_scope('loss'):
        number_of_samples = tf.shape(labels)[0]
        total_loss = tf.reduce_sum(tf.nn.weighted_cross_entropy_with_logits(
            targets=labels, logits=predictions, pos_weight=positive_upweight))
        return total_loss / tf.cast(number_of_samples, tf.float32)


def _get_train_op(optimizer, loss_op):
    """Perform gradient updates on model.

    :param loss_op: tensorflow loss op representing loss for which to compute gradients
    :return: tensorflow training op
    """
    with tf.name_scope('optimizer'):
        train_op = optimizer.minimize(loss_op)
        return train_op


def _get_summary_op(loss_op):
    """Summarize training statistics.

    :param loss_op: tensorflow loss op representing loss for which to compute gradients
    :return: tensorflow summary op
    """
    with tf.name_scope("summaries"):
        tf.summary.scalar("loss", loss_op)
        tf.summary.histogram("histogram_loss", loss_op)
        return tf.summary.merge_all()


def _infer_training_parameters(dataset, task_index):
    """Infer training parameters from dataset and task.

    Currently, the only training parameter we infer is the upweight parameter applied to positive samples.

    :param dataset: tf_dataset object
    :param task_index: task for which to compute statistics.
    :param sess: tensorflow session 
    :return: float. upweight parameter for positive samples
    """
    with tf.Session() as sess:
        return infer_positive_upweight_parameter(dataset=dataset, task_index=task_index, sess=sess)

