import collections
import numpy as np
import tensorflow as tf

from attention_for_histone_modification.libs.trainer.abstract_trainer import AbstractTrainer
from attention_for_histone_modification.libs.preprocessing.utilities import partition_indices, get_shuffled_indices

class AttentionTrainer(AbstractTrainer):
    """Trainer implementation for training attention models."""

    def __init__(self):
        """Initialize trainer."""
        self._epochs = 1

    def train_model(self, dataset, model, optimizer):
        """Train attention model."""
        
        graph_inputs, ops = build_computational_graph(model, optimizer)

        ## initialize variables
        init_op = tf.global_variables_initializer()
        tf.get_variable_scope().reuse_variables()

        ## initialize session and start training
        with tf.Session() as sess:
            sess.run(init_op)

            for _ in xrange(self._epochs):
                train_epoch(graph_inputs, ops, dataset, sess)


def build_computational_graph(model, optimizer):
    """Construct a computational graph for training a model.

    :param model: tensorflow model to be trained
    :param optimizer: optimizer for gradient backpropogation
    :return: 
        2-tuple consisting the two dictionaries. The first dictionary contains tf.placeholders
        representing inputs to the graph. The second dictionary contains ops generated by the graph.
    """
    graph_inputs = {'features'  : model.inputs['features'],
                    'sequences' : model.inputs['sequences'],
                    'labels'    : model.outputs['labels']}

    predictions = model.predict(features=graph_inputs['features'], 
                                sequences=graph_inputs['sequences'])

    loss_op = get_loss_op(predictions=predictions, labels=graph_inputs['labels'])
    train_op = get_train_op(loss_op=loss_op, optimizer=optimizer)

    op_dictionary = {'loss_op' : loss_op, 'train_op': train_op}
    return graph_inputs, op_dictionary


def train_epoch(graph_inputs, op_dictionary, dataset, sess):
    """Execute one epoch of training.
    
    :param graph_inputs: dictionary mapping string keys to tf.placeholders in computational graph.
    :param op_dictionary: dictionary mapping string keys to tensorflow ops.
    """
    batch_size = 100
    epoch_batches, _ = partition_indices(get_shuffled_indices(dataset.total_examples), batch_size)
    for iteration_number, batch_indices in enumerate(epoch_batches):
        _, training_examples = zip(*dataset.get_training_examples(batch_indices))

        training_tensor = convert_to_training_tensor(training_examples)
        
        feed_dict = {graph_inputs['sequences']: training_tensor.sequence_tensor,
                     graph_inputs['features']: training_tensor.annotation_tensor,
                     graph_inputs['labels']: training_tensor.label_tensor}
        
        _, loss_value = sess.run([op_dictionary['train_op'], op_dictionary['loss_op']], feed_dict)
        print "the loss for iteration {} = {}".format(iteration_number, loss_value)
        

def get_loss_op(predictions, labels):
    """Return loss for model.

    :param predictions: tensor ouput from model.
    :param labels: groundtruth labels.
    :return: loss
    """
    total_loss = tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(logits=predictions, labels=labels))
    batch_size = 100
    return total_loss / batch_size 


def get_train_op(optimizer, loss_op):
    """Perform gradient updates on model.

    :param loss_op: tensorflow loss op representing loss for which to compute gradients
    :return: tensorflow training op
    """
    with tf.name_scope('optimizer'):
        train_op = optimizer.minimize(loss_op)
        return train_op

TrainingTensor = collections.namedtuple(typename="TrainingTensor", 
                                        field_names=['sequence_tensor', 'annotation_tensor', 'label_tensor'])

def convert_to_training_tensor(training_examples):
    """Convert training examples to training tensor for tf model.
    
    :param training_examples:
        List of attention training examples.
    :return:
        TrainingTensor object.
    """
    sequence_tensor = np.concatenate([np.expand_dims(te.sequence, axis=0) for te in training_examples], axis=0)
    label_tensor = np.concatenate([np.expand_dims(te.label, axis=0) for te in training_examples], axis=0)
    annotation_tensor = np.concatenate([np.reshape(te.annotation, (1, 1, te.annotation.size)) for te in training_examples], axis=0)
    return TrainingTensor(sequence_tensor=sequence_tensor, annotation_tensor=annotation_tensor, label_tensor=label_tensor)

