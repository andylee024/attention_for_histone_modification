import abc
import os
import tensorflow as tf

from komorebi.libs.trainer.abstract_trainer import AbstractTrainer
from komorebi.libs.trainer.trainer_config import TrainerConfiguration
from komorebi.libs.trainer.trainer_utils import batch_data
from komorebi.libs.utilities.io_utils import ensure_directory

CHECKPOINT_DIRECTORY_NAME = "model_checkpoints"
SUMMARY_DIRECTORY_NAME = "training_summaries"

class AbstractTensorflowTrainer(AbstractTrainer):
    """Abstract base class to facilitate training models specific to tensorflow."""

    __metaclass__ = abc.ABCMeta

    def __init__(self, config):
        """Initialize trainer.
        
        :param config: trainer_config object.
        """
        assert isinstance(config, TrainerConfiguration)
        ensure_directory(config.experiment_directory)
        self._checkpoint_directory, self._summary_directory = _create_training_directories(
                os.path.abspath(config.experiment_directory))

        self.epochs = config.epochs
        self.batch_size = config.batch_size
        self.checkpoint_frequency = config.checkpoint_frequency
        self.model_checkpoint_path = os.path.join(self._checkpoint_directory, "model_checkpoint") 


    def train_model(self, model, dataset, optimizer):
        """Training procedure for a tensorflow model.

        This is a generic training procedure for a tensorflow model. Specifically, it 
        supports training a model and logging intermediate output to the directories
        specified below. 

        |-- base_directory
            |-- model_training_directory
                |-- checkpoints
                |-- final_model.pb
                |-- summaries
        
        :param model: model object satisfying abstract model interface
        :param dataset: dataset object satisfying abstract dataset interface
        """
        # build computational model
        graph_inputs, ops = self._build_computational_graph(model, optimizer)

        # initialization 
        init_op = tf.global_variables_initializer()
        tf.get_variable_scope().reuse_variables()

        saver = tf.train.Saver()
        
        # initialize session and start training
        with tf.Session() as sess:
            sess.run(init_op)
            for epoch in xrange(self.epochs):
                
                # training
                _train_epoch(dataset=dataset,
                             batch_size=self.batch_size,
                             graph_inputs=graph_inputs,
                             ops=ops, 
                             convert_training_examples=self._convert_training_examples_to_feed_dict,
                             sess=sess)

                # checkpoint saving 
                if (epoch % self.checkpoint_frequency == 0):
                    saver.save(sess=sess, save_path=self.model_checkpoint_path, global_step=epoch)
                    print "saved: {}-{}".format(self.model_checkpoint_path, epoch)


    @abc.abstractmethod
    def _build_computational_graph(self, model, optimizer):
        """Construct a computational graph for training a model.

        :param model: tensorflow model to be trained
        :param optimizer: optimizer for gradient backpropogation
        :return: 
            2-tuple consisting the two dictionaries. The first dictionary contains tf.placeholders
            representing inputs to the graph. The second dictionary contains ops generated by the graph.
        """
        pass


    @abc.abstractmethod
    def _convert_training_examples_to_feed_dict(self, graph_inputs, training_examples):
        """Convert training inputs to graph inputs.

        Tensorflow models rely on passing a feed_dict into the computational graph.
        This function is responsible for translating the training examples into graph inputs.

        :param graph_inputs: dictionary mapping from string key to tf.placeholders
        :param training_examples: training example types specific to dataset.
        """
        pass


def _train_epoch(dataset, batch_size, graph_inputs, ops, convert_training_examples, sess):
    """Execute training for one epoch.
    
    :param dataset: dataset to train on
    :param graph_inputs: 
    :param batch_size: size of each batch in iteration
    :param ops: dictionary of ops from computational graph
    :param convert_training_examples: function to convert training examples to feed dict.
    :param sess: tensorflow session
    """
    training_batches = batch_data(dataset, batch_size=batch_size)
    for idx, training_batch in enumerate(training_batches):
       _, loss = sess.run(fetches=[ops['train_op'], ops['loss_op']], 
                          feed_dict=convert_training_examples(graph_inputs, training_batch))

"""
            # Create prediction signature
            tensor_info_features = tf.saved_model.utils.build_tensor_info(graph_inputs['features'])
            tensor_info_sequences = tf.saved_model.utils.build_tensor_info(graph_inputs['sequences'])
            tensor_info_predictions = tf.saved_model.utils.build_tensor_info(model.predict(graph_inputs['features'], graph_inputs['sequences']).predictions)

            prediction_signature = tf.saved_model.signature_def_utils.build_signature_def(
                    inputs={'features': tensor_info_features, 'sequences': tensor_info_sequences},
                    outputs={'predictions': tensor_info_predictions},
                    method_name="prediction_signature")
                
            # Saving
            import os
            export_dir = os.path.join(self.save_directory, "final_save_model_directory")
            builder = tf.saved_model.builder.SavedModelBuilder(export_dir)
            builder.add_meta_graph_and_variables(sess,
                                                 ["tag"], 
                                                 signature_def_map={"predict": prediction_signature})
            builder.save()
            print "saved final model"

            /private/tmp/tf_checkpoints/final_save_model_directory/saved_model.pb
"""

def _create_training_directories(experiment_directory):
    """Create directories for recording data from training procedure.

    :param experiment_directory: base experiment directory
    :return: 
        2-tuple (checkpoint_directory, summary_directory), where the checkpoint directory stores
        model checkpoints and the summary directory stores tensorflow summary files.
    """
    checkpoint_directory = os.path.join(experiment_directory, CHECKPOINT_DIRECTORY_NAME)
    summary_directory = os.path.join(experiment_directory, SUMMARY_DIRECTORY_NAME)
    ensure_directory(checkpoint_directory)
    ensure_directory(summary_directory)
    return checkpoint_directory, summary_directory

